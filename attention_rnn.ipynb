{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6/EkmEvjvaj0lMt4/SQDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-kanaan/generative-ai/blob/master/attention_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1lBwt6lwWFb",
        "outputId": "b43e9731-1de0-41d9-d19a-bb1990ff8039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bank in financial context: [ 0.6620845  -0.41482687  1.5661771   1.0931777  -0.10954762  0.5104228 ]\n",
            "Bank in river context: [ 0.6620845  -0.41482687  1.5661771   1.0931777  -0.10954762  0.5104228 ]\n",
            "Cosine similarity: 1.0000001192092896\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a mini RNN model\n",
        "class MiniRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(MiniRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        return embedded, output\n",
        "\n",
        "# Define vocabulary\n",
        "vocab = {\"the\": 0, \"bank\": 1, \"money\": 2, \"river\": 3}\n",
        "sentences = [\n",
        "    [\"the\", \"bank\", \"money\"],  # financial context\n",
        "    [\"the\", \"bank\", \"river\"]   # river context\n",
        "]\n",
        "\n",
        "# Convert to tensor\n",
        "encoded = [[vocab[word] for word in sentence] for sentence in sentences]\n",
        "input_tensor = torch.tensor(encoded)\n",
        "\n",
        "# Initialize model\n",
        "model = MiniRNN(vocab_size=4, embedding_dim=6, hidden_dim=8)\n",
        "\n",
        "# Run the model\n",
        "embeddings, outputs = model(input_tensor)\n",
        "\n",
        "# Extract 'bank' embeddings\n",
        "bank_financial = embeddings[0, 1]\n",
        "bank_river = embeddings[1, 1]\n",
        "\n",
        "# Print and compare\n",
        "print(\"Bank in financial context:\", bank_financial.detach().numpy())\n",
        "print(\"Bank in river context:\", bank_river.detach().numpy())\n",
        "print(\"Cosine similarity:\", nn.functional.cosine_similarity(bank_financial, bank_river, dim=0).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define toy vocabulary\n",
        "vocab = {\"the\": 0, \"bank\": 1, \"money\": 2, \"river\": 3}\n",
        "sentences = [\n",
        "    [\"the\", \"bank\", \"money\"],  # financial context\n",
        "    [\"the\", \"bank\", \"river\"]   # river context\n",
        "]\n",
        "encoded = [[vocab[word] for word in sentence] for sentence in sentences]\n",
        "inputs = torch.tensor(encoded)  # shape [2, 3]\n",
        "\n",
        "# Define RNN encoder + attention mechanism\n",
        "class AttentionRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(AttentionRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.attn = nn.Linear(hidden_dim, 1)  # computes attention score for each time step\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)            # shape: [batch, seq_len, embed_dim]\n",
        "        outputs, _ = self.rnn(embedded)          # shape: [batch, seq_len, hidden_dim]\n",
        "\n",
        "        # Apply attention over time steps\n",
        "        scores = self.attn(outputs).squeeze(-1)  # shape: [batch, seq_len]\n",
        "        weights = F.softmax(scores, dim=1)       # normalized attention weights\n",
        "\n",
        "        # Weighted sum of hidden states\n",
        "        context = torch.bmm(weights.unsqueeze(1), outputs)  # shape: [batch, 1, hidden_dim]\n",
        "        return context.squeeze(1), weights  # final embedding, attention weights\n",
        "\n",
        "# Initialize and run model\n",
        "model = AttentionRNN(vocab_size=4, embed_dim=6, hidden_dim=8)\n",
        "contextual_embedding, attention_weights = model(inputs)\n",
        "\n",
        "# Print attention weights\n",
        "print(\"\\n--- Attention Weights ---\")\n",
        "for i, context in enumerate([\"financial\", \"river\"]):\n",
        "    print(f\"{context} context:\", attention_weights[i].detach().numpy())\n",
        "\n",
        "# Print final embedding (context vector)\n",
        "print(\"\\n--- Final Embeddings (after attention) ---\")\n",
        "print(\"Financial context:\", contextual_embedding[0].detach().numpy())\n",
        "print(\"River context:\", contextual_embedding[1].detach().numpy())\n",
        "\n",
        "# Check similarity\n",
        "cos_sim = F.cosine_similarity(contextual_embedding[0], contextual_embedding[1], dim=0)\n",
        "print(f\"\\nCosine similarity between 'bank' in different contexts: {cos_sim.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYlPCxaTyPAk",
        "outputId": "834dd3a2-b580-46d2-ad18-1361c8b60475"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Attention Weights ---\n",
            "financial context: [0.38765308 0.22650419 0.38584268]\n",
            "river context: [0.45152026 0.26382154 0.28465822]\n",
            "\n",
            "--- Final Embeddings (after attention) ---\n",
            "Financial context: [-0.30122727  0.11949204 -0.42871952  0.671422    0.6595087   0.10575585\n",
            "  0.47658813  0.6702254 ]\n",
            "River context: [-0.36939737  0.18260762 -0.3525775   0.723035    0.456938   -0.11162592\n",
            "  0.6054571   0.45157906]\n",
            "\n",
            "Cosine similarity between 'bank' in different contexts: 0.9536\n"
          ]
        }
      ]
    }
  ]
}