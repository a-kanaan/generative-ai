{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b85b17f-5b53-4ee2-9e01-1a6e5250758c",
   "metadata": {},
   "source": [
    "## Phi-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab8efd-4224-4c1c-be36-354b3398337c",
   "metadata": {},
   "source": [
    "# 1. Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8022df-dc66-452f-b977-6487b60afaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677b8ea-cefc-403b-9730-58cc42022221",
   "metadata": {},
   "source": [
    "# 2. Manual Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420dbe4c-7c47-4a95-a17a-c63afe6fea24",
   "metadata": {},
   "source": [
    "## 2.1. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b458554-de6c-432e-8942-6fab60224ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024007081985473633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8218fb09fe4e12856baf5a63a3fe95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load the causal language model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d53c72-55be-4917-8d44-d60cdeeaf5fd",
   "metadata": {},
   "source": [
    "## 2.2. Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fce2f45-9910-4510-aec9-75b2219f4401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15043 --> ▁Hello\n",
      "3186 --> ▁world\n"
     ]
    }
   ],
   "source": [
    "# Encode text into input IDs\n",
    "encoding = tokenizer(text)\n",
    "\n",
    "# Convert input IDs to readable tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "\n",
    "# Print token IDs and their corresponding tokens\n",
    "for token_id, token_str in zip(encoding['input_ids'], tokens):\n",
    "    print(f\"{token_id} --> {token_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3576b2f-23f1-426d-8e62-14a36bf93494",
   "metadata": {},
   "source": [
    "## 2.3. Model Inference (Logits and Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0a344a1-04c5-4520-847d-b7da301e70c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "tensor(48., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "tensor(29892, device='cuda:0')\n",
      "tensor(29991, device='cuda:0')\n",
      ",!\n",
      ",!\n",
      ",!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and move input to same device as model\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Get raw output logits from model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Print max logit values (confidence per token position)\n",
    "print(outputs.logits[0][0].max())  # For first token\n",
    "print(outputs.logits[0][1].max())  # For second token\n",
    "\n",
    "# Print predicted token IDs (most likely next token at each position)\n",
    "print(outputs.logits[0][0].argmax())  # ID of most likely first token\n",
    "print(outputs.logits[0][1].argmax())  # ID of most likely second token\n",
    "\n",
    "# Decode a couple of token IDs manually\n",
    "print(tokenizer.decode([29892, 29991]))\n",
    "print(tokenizer.decode(outputs.logits[0].argmax(-1)))\n",
    "\n",
    "# Full predicted token sequence from logits\n",
    "predicted_token_id = outputs.logits.argmax(dim=-1)\n",
    "print(tokenizer.decode(predicted_token_id[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5eff03-2bb8-4d15-9288-9d24436f23e3",
   "metadata": {},
   "source": [
    "## 2.4. Generate continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "525fb8d7-8d13-43dc-93f0-db6fe5e2aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! This is a simple function demonstrating how you can print \"Hello World!\" in Python\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation\n",
    "generated_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    #max_new_tokens=50,       # Number of tokens to generate\n",
    "    do_sample=True,         # Use greedy decoding (set True for randomness)\n",
    "    pad_token_id=tokenizer.eos_token_id  # Avoid warning if eos is not set\n",
    ")\n",
    "\n",
    "# Decode and print result\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14017ff5-6267-4648-8352-0e5918ebaeec",
   "metadata": {},
   "source": [
    "# 3. Pipeline Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192d2b-3669-4c7b-b5a2-2318a8895fad",
   "metadata": {},
   "source": [
    "## 3.1. Pipeline Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec88d546-0390-41e4-9af6-e8d271a73899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text-generation pipeline using the same model and tokenizer\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,   # Limit length of generated text\n",
    "    do_sample=False       # Deterministic (no sampling)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff5493-86d9-4785-b703-89432b810d3e",
   "metadata": {},
   "source": [
    "## 3.2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a64c4ebd-b22b-49d4-9668-36c7b6d5f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This instruction is a simple greeting in English, mirroring the format and difficulty of the given Chinese instruction. The task is to generate a similar greeting in English.\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello World\"}\n",
    "]\n",
    "\n",
    "# Generate text using the prompt\n",
    "output = generator(messages)\n",
    "\n",
    "# Print generated output\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b22261cc-abf1-47a0-9835-359b50356469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the chicken join the band? Because it had the drumsticks!\n"
     ]
    }
   ],
   "source": [
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e680e95-d758-4bdb-9f11-1075130a38d6",
   "metadata": {},
   "source": [
    "## 3.3. Small Chatbot! Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d670867-6a00-41f3-b04d-1fecdb0ad5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation has ended!\n"
     ]
    }
   ],
   "source": [
    "# Store chat history\n",
    "messages = []\n",
    "\n",
    "print(\"Chatbot is ready! Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print('Conversation has ended!')\n",
    "        break\n",
    "\n",
    "    # Add user message to history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate model response\n",
    "    response = generator(messages)\n",
    "\n",
    "    # Get the assistant reply\n",
    "    reply = response[0][\"generated_text\"]\n",
    "    print(\"Bot:\", reply)\n",
    "\n",
    "    # Add assistant reply to history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eocv",
   "language": "python",
   "name": "eocv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
